{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorboard.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MZW2Cb1qlYnNOpaWK5y_9TORD8w5_qYd",
      "authorship_tag": "ABX9TyMpyP2qA4N9euVlhJoxhIdt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profshai/Python-MachineLearning-DeepLearning-Projects/blob/master/Tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXM_bFkoKaei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsJaDSAtKnzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "a43cc299-ea66-4211-af5b-4ab73f0a83e1"
      },
      "source": [
        "df = pd.read_csv('drive/My Drive/udemytfdata/DATA/cancer_classification.csv')\n",
        "df"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>benign_0__mal_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean radius  mean texture  ...  worst fractal dimension  benign_0__mal_1\n",
              "0          17.99         10.38  ...                  0.11890                0\n",
              "1          20.57         17.77  ...                  0.08902                0\n",
              "2          19.69         21.25  ...                  0.08758                0\n",
              "3          11.42         20.38  ...                  0.17300                0\n",
              "4          20.29         14.34  ...                  0.07678                0\n",
              "..           ...           ...  ...                      ...              ...\n",
              "564        21.56         22.39  ...                  0.07115                0\n",
              "565        20.13         28.25  ...                  0.06637                0\n",
              "566        16.60         28.08  ...                  0.07820                0\n",
              "567        20.60         29.33  ...                  0.12400                0\n",
              "568         7.76         24.54  ...                  0.07039                1\n",
              "\n",
              "[569 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIXzBp8iK5On",
        "colab_type": "text"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCE0mxtyOXqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop('benign_0__mal_1',axis=1).values\n",
        "y = df['benign_0__mal_1'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unfq15qkOaeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCLhBx_pOcix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJWXNkcKOhDn",
        "colab_type": "text"
      },
      "source": [
        "### Scaling Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbg8KAi4Oj_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el8vR_QROnEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx_5nqbXOph-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d3a1015-9803-4b2c-d892-bf5fe39f7442"
      },
      "source": [
        "scaler.fit(X_train)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmZNb8cBOrom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55XEZm1YOu3H",
        "colab_type": "text"
      },
      "source": [
        "### Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lILMh8A8Oyu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_G0t80NO1-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_ujrUfwO6Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qulL9UQcO8pT",
        "colab_type": "text"
      },
      "source": [
        "### Creating the Tensorboard Callback\n",
        "\n",
        "TensorBoard is a visualization tool provided with TensorFlow.\n",
        "\n",
        "This callback logs events for TensorBoard, including:\n",
        "* Metrics summary plots\n",
        "* Training graph visualization\n",
        "* Activation histograms\n",
        "* Sampled profiling\n",
        "\n",
        "If you have installed TensorFlow with pip, you should be able\n",
        "to launch TensorBoard from the command line:\n",
        "\n",
        "```sh\n",
        "tensorboard --logdir=path_to_your_logs\n",
        "```\n",
        "\n",
        "You can find more information about TensorBoard\n",
        "[here](https://www.tensorflow.org/tensorboard/).\n",
        "\n",
        "    Arguments:\n",
        "        log_dir: the path of the directory where to save the log files to be\n",
        "          parsed by TensorBoard.\n",
        "        histogram_freq: frequency (in epochs) at which to compute activation and\n",
        "          weight histograms for the layers of the model. If set to 0, histograms\n",
        "          won't be computed. Validation data (or split) must be specified for\n",
        "          histogram visualizations.\n",
        "        write_graph: whether to visualize the graph in TensorBoard. The log file\n",
        "          can become quite large when write_graph is set to True.\n",
        "        write_images: whether to write model weights to visualize as image in\n",
        "          TensorBoard.\n",
        "        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n",
        "          writes the losses and metrics to TensorBoard after each batch. The same\n",
        "          applies for `'epoch'`. If using an integer, let's say `1000`, the\n",
        "          callback will write the metrics and losses to TensorBoard every 1000\n",
        "          samples. Note that writing too frequently to TensorBoard can slow down\n",
        "          your training.\n",
        "        profile_batch: Profile the batch to sample compute characteristics. By\n",
        "          default, it will profile the second batch. Set profile_batch=0 to\n",
        "          disable profiling. Must run in TensorFlow eager mode.\n",
        "        embeddings_freq: frequency (in epochs) at which embedding layers will\n",
        "          be visualized. If set to 0, embeddings won't be visualized.\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OQV15wYO-1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8ESQ6_iPPTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f63dfa8e-f902-4798-eae5-99ca9ef22446"
      },
      "source": [
        "datetime.now().strftime(\"%Y-%m-%d--%H%M\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2020-05-02--2155'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JPV_s59PRZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WINDOWS: Use \"logs\\\\fit\"\n",
        "# MACOS/LINUX: Use \"logs\\fit\"\n",
        "\n",
        "log_directory = 'logs\\\\fit'\n",
        "\n",
        "# OPTIONAL: ADD A TIMESTAMP FOR UNIQUE FOLDER\n",
        "# timestamp = datetime.now().strftime(\"%Y-%m-%d--%H%M\")\n",
        "# log_directory = log_directory + '\\\\' + timestamp\n",
        "\n",
        "\n",
        "board = TensorBoard(log_dir=log_directory,histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    write_images=True,\n",
        "    update_freq='epoch',\n",
        "    profile_batch=2,\n",
        "    embeddings_freq=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90qJOhttPX4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=30,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=15,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NYpI0wQPgPm",
        "colab_type": "text"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3JrBYSOPmMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a35f2bf-4e2f-4138-b77f-6514b7c41c32"
      },
      "source": [
        "model.fit(x=X_train, \n",
        "          y=y_train, \n",
        "          epochs=600,\n",
        "          validation_data=(X_test, y_test), verbose=1,\n",
        "          callbacks=[early_stop,board]\n",
        "          )"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.6915 - val_loss: 0.6726\n",
            "Epoch 2/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.6738 - val_loss: 0.6576\n",
            "Epoch 3/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.6631 - val_loss: 0.6381\n",
            "Epoch 4/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.6500 - val_loss: 0.6150\n",
            "Epoch 5/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.6252 - val_loss: 0.5841\n",
            "Epoch 6/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5930 - val_loss: 0.5524\n",
            "Epoch 7/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5875 - val_loss: 0.5199\n",
            "Epoch 8/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5778 - val_loss: 0.4948\n",
            "Epoch 9/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5314 - val_loss: 0.4696\n",
            "Epoch 10/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5047 - val_loss: 0.4405\n",
            "Epoch 11/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.5002 - val_loss: 0.4116\n",
            "Epoch 12/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4827 - val_loss: 0.3893\n",
            "Epoch 13/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4591 - val_loss: 0.3694\n",
            "Epoch 14/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.4302 - val_loss: 0.3443\n",
            "Epoch 15/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3899 - val_loss: 0.3210\n",
            "Epoch 16/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.4004 - val_loss: 0.2960\n",
            "Epoch 17/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3835 - val_loss: 0.2764\n",
            "Epoch 18/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3674 - val_loss: 0.2637\n",
            "Epoch 19/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3578 - val_loss: 0.2509\n",
            "Epoch 20/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3803 - val_loss: 0.2415\n",
            "Epoch 21/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3162 - val_loss: 0.2310\n",
            "Epoch 22/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3642 - val_loss: 0.2180\n",
            "Epoch 23/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.3248 - val_loss: 0.2137\n",
            "Epoch 24/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3016 - val_loss: 0.2038\n",
            "Epoch 25/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.3175 - val_loss: 0.1980\n",
            "Epoch 26/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2741 - val_loss: 0.1859\n",
            "Epoch 27/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2718 - val_loss: 0.1829\n",
            "Epoch 28/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2721 - val_loss: 0.1779\n",
            "Epoch 29/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2656 - val_loss: 0.1638\n",
            "Epoch 30/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2267 - val_loss: 0.1554\n",
            "Epoch 31/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2570 - val_loss: 0.1488\n",
            "Epoch 32/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2627 - val_loss: 0.1461\n",
            "Epoch 33/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2545 - val_loss: 0.1602\n",
            "Epoch 34/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2446 - val_loss: 0.1416\n",
            "Epoch 35/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2612 - val_loss: 0.1408\n",
            "Epoch 36/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1994 - val_loss: 0.1521\n",
            "Epoch 37/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2363 - val_loss: 0.1332\n",
            "Epoch 38/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2172 - val_loss: 0.1288\n",
            "Epoch 39/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2380 - val_loss: 0.1272\n",
            "Epoch 40/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2016 - val_loss: 0.1295\n",
            "Epoch 41/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.2095 - val_loss: 0.1331\n",
            "Epoch 42/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2089 - val_loss: 0.1196\n",
            "Epoch 43/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1958 - val_loss: 0.1168\n",
            "Epoch 44/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1984 - val_loss: 0.1210\n",
            "Epoch 45/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.2068 - val_loss: 0.1255\n",
            "Epoch 46/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1829 - val_loss: 0.1136\n",
            "Epoch 47/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1774 - val_loss: 0.1083\n",
            "Epoch 48/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1950 - val_loss: 0.1064\n",
            "Epoch 49/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1920 - val_loss: 0.1161\n",
            "Epoch 50/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1557 - val_loss: 0.1077\n",
            "Epoch 51/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1778 - val_loss: 0.1006\n",
            "Epoch 52/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1639 - val_loss: 0.0984\n",
            "Epoch 53/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1674 - val_loss: 0.1001\n",
            "Epoch 54/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1585 - val_loss: 0.0985\n",
            "Epoch 55/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1748 - val_loss: 0.0973\n",
            "Epoch 56/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1585 - val_loss: 0.1095\n",
            "Epoch 57/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1656 - val_loss: 0.0915\n",
            "Epoch 58/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1685 - val_loss: 0.1113\n",
            "Epoch 59/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1331 - val_loss: 0.0963\n",
            "Epoch 60/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1387 - val_loss: 0.0965\n",
            "Epoch 61/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1601 - val_loss: 0.0937\n",
            "Epoch 62/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1370 - val_loss: 0.1003\n",
            "Epoch 63/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1602 - val_loss: 0.0889\n",
            "Epoch 64/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1606 - val_loss: 0.0950\n",
            "Epoch 65/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1563 - val_loss: 0.1017\n",
            "Epoch 66/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1537 - val_loss: 0.0955\n",
            "Epoch 67/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1696 - val_loss: 0.0966\n",
            "Epoch 68/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1240 - val_loss: 0.0927\n",
            "Epoch 69/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1321 - val_loss: 0.0909\n",
            "Epoch 70/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1473 - val_loss: 0.0878\n",
            "Epoch 71/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1237 - val_loss: 0.0924\n",
            "Epoch 72/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1351 - val_loss: 0.0884\n",
            "Epoch 73/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1254 - val_loss: 0.0974\n",
            "Epoch 74/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1312 - val_loss: 0.1049\n",
            "Epoch 75/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1299 - val_loss: 0.0922\n",
            "Epoch 76/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1228 - val_loss: 0.0890\n",
            "Epoch 77/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1058 - val_loss: 0.0960\n",
            "Epoch 78/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1167 - val_loss: 0.0852\n",
            "Epoch 79/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1183 - val_loss: 0.0930\n",
            "Epoch 80/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1285 - val_loss: 0.0874\n",
            "Epoch 81/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1463 - val_loss: 0.0823\n",
            "Epoch 82/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1423 - val_loss: 0.0998\n",
            "Epoch 83/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1137 - val_loss: 0.0973\n",
            "Epoch 84/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1349 - val_loss: 0.0869\n",
            "Epoch 85/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1177 - val_loss: 0.0822\n",
            "Epoch 86/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1050 - val_loss: 0.0975\n",
            "Epoch 87/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1110 - val_loss: 0.0968\n",
            "Epoch 88/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1328 - val_loss: 0.0874\n",
            "Epoch 89/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1085 - val_loss: 0.0875\n",
            "Epoch 90/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1147 - val_loss: 0.0818\n",
            "Epoch 91/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1027 - val_loss: 0.0837\n",
            "Epoch 92/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0911 - val_loss: 0.0856\n",
            "Epoch 93/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1022 - val_loss: 0.0882\n",
            "Epoch 94/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0911 - val_loss: 0.0966\n",
            "Epoch 95/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1079 - val_loss: 0.0855\n",
            "Epoch 96/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1149 - val_loss: 0.0911\n",
            "Epoch 97/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1265 - val_loss: 0.0807\n",
            "Epoch 98/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1191 - val_loss: 0.0809\n",
            "Epoch 99/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0996 - val_loss: 0.0927\n",
            "Epoch 100/600\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.1134 - val_loss: 0.0983\n",
            "Epoch 101/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0965 - val_loss: 0.0811\n",
            "Epoch 102/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0914 - val_loss: 0.0794\n",
            "Epoch 103/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0900 - val_loss: 0.0815\n",
            "Epoch 104/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0843 - val_loss: 0.0768\n",
            "Epoch 105/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0988 - val_loss: 0.0781\n",
            "Epoch 106/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0977 - val_loss: 0.0813\n",
            "Epoch 107/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1027 - val_loss: 0.0841\n",
            "Epoch 108/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1013 - val_loss: 0.0834\n",
            "Epoch 109/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1287 - val_loss: 0.0835\n",
            "Epoch 110/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1192 - val_loss: 0.0791\n",
            "Epoch 111/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1217 - val_loss: 0.0962\n",
            "Epoch 112/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1093 - val_loss: 0.0883\n",
            "Epoch 113/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0800 - val_loss: 0.0844\n",
            "Epoch 114/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1024 - val_loss: 0.0966\n",
            "Epoch 115/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.0898\n",
            "Epoch 116/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0814 - val_loss: 0.0823\n",
            "Epoch 117/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0967 - val_loss: 0.0827\n",
            "Epoch 118/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0866 - val_loss: 0.0817\n",
            "Epoch 119/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1017 - val_loss: 0.0972\n",
            "Epoch 120/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1083 - val_loss: 0.0817\n",
            "Epoch 121/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1039 - val_loss: 0.0835\n",
            "Epoch 122/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.1259 - val_loss: 0.0796\n",
            "Epoch 123/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0939 - val_loss: 0.1056\n",
            "Epoch 124/600\n",
            "14/14 [==============================] - 0s 6ms/step - loss: 0.0945 - val_loss: 0.0870\n",
            "Epoch 125/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0882 - val_loss: 0.0833\n",
            "Epoch 126/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.1071 - val_loss: 0.0796\n",
            "Epoch 127/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0864 - val_loss: 0.0876\n",
            "Epoch 128/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0837 - val_loss: 0.0812\n",
            "Epoch 129/600\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 0.0885 - val_loss: 0.0825\n",
            "Epoch 00129: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66fc4ebdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTpo9zBhPpis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ad1cf2b-65e0-4056-da0e-84c659411a43"
      },
      "source": [
        "### Running Tensorboard\n",
        "print(log_directory)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logs\\fit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfbqUFa5VeK5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60b34ad4-434e-463b-9f16-b0876413c58e"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOepncezSGJc",
        "colab_type": "text"
      },
      "source": [
        "End of Notebook!"
      ]
    }
  ]
}